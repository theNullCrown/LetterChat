{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import json\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import BertForSequenceClassification, BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your dataset class\n",
    "class PromptDataset(Dataset):\n",
    "    def __init__(self, file_path):\n",
    "        self.data = self.load_data(file_path)\n",
    "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        prompt = self.data[idx]['prompt']\n",
    "        label = self.data[idx]['label']\n",
    "\n",
    "        if label == \"generation\":\n",
    "            label_encoded = 0\n",
    "        elif label == \"completion\":\n",
    "            label_encoded = 1\n",
    "        elif label == \"question-answer\":\n",
    "            label_encoded = 2\n",
    "\n",
    "        # Tokenize the prompt\n",
    "        encoded_prompt = self.tokenizer.encode_plus(\n",
    "            prompt,\n",
    "            add_special_tokens=True,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=128,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoded_prompt['input_ids'].squeeze(),\n",
    "            'attention_mask': encoded_prompt['attention_mask'].squeeze(),\n",
    "            'label': torch.tensor(label_encoded)\n",
    "        }\n",
    "    \n",
    "    def load_data(self, file_path):\n",
    "        with open(file_path, 'r') as file:\n",
    "            data = json.load(file)\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = PromptDataset('../data/prompts.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20 - Average Loss: 1.1280\n",
      "Epoch 2/20 - Average Loss: 0.8706\n",
      "Epoch 3/20 - Average Loss: 0.8441\n",
      "Epoch 4/20 - Average Loss: 0.7217\n",
      "Epoch 5/20 - Average Loss: 0.6683\n",
      "Epoch 6/20 - Average Loss: 0.5854\n",
      "Epoch 7/20 - Average Loss: 0.4398\n",
      "Epoch 8/20 - Average Loss: 0.4632\n",
      "Epoch 9/20 - Average Loss: 0.3556\n",
      "Epoch 10/20 - Average Loss: 0.2946\n",
      "Epoch 11/20 - Average Loss: 0.3020\n",
      "Epoch 12/20 - Average Loss: 0.2267\n",
      "Epoch 13/20 - Average Loss: 0.2160\n",
      "Epoch 14/20 - Average Loss: 0.1726\n",
      "Epoch 15/20 - Average Loss: 0.1532\n",
      "Epoch 16/20 - Average Loss: 0.1358\n",
      "Epoch 17/20 - Average Loss: 0.1173\n",
      "Epoch 18/20 - Average Loss: 0.1129\n",
      "Epoch 19/20 - Average Loss: 0.0949\n",
      "Epoch 20/20 - Average Loss: 0.0936\n"
     ]
    }
   ],
   "source": [
    "# Define the model architecture\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=3)\n",
    "\n",
    "# Define your training parameters\n",
    "batch_size = 32\n",
    "num_epochs = 20\n",
    "learning_rate = 2e-5\n",
    "\n",
    "# Create a DataLoader for batching and shuffling the data\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Set device (GPU if available, else CPU)\n",
    "device = torch.device('mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "\n",
    "# Move the model to the device\n",
    "model = model.to(device)\n",
    "\n",
    "# Set the optimizer and loss function\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Training loop\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    for batch in dataloader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "        \n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        logits = outputs.logits\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    average_loss = total_loss / len(dataloader)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} - Average Loss: {average_loss:.4f}\")\n",
    "\n",
    "# Save the trained model\n",
    "model.save_pretrained('../models/prompt_classifier')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Label: 2\n"
     ]
    }
   ],
   "source": [
    "##Testing\n",
    "# Load the saved model\n",
    "model = BertForSequenceClassification.from_pretrained('../models/prompt_classifier/')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Example input\n",
    "input_text = \"Dear Darren, I am writing to express my interest in the software engineer position at Amazon. In my previous role as a software developer, I successfully completed several projects that required me to collaborate with cross-functional teams. I am confident in my ability to contribute to your organization's growth and deliver high-quality code. What type of projects did you work on as a software developer that involved collaboration with cross-functional teams?\"\n",
    "\n",
    "# Tokenize the input text\n",
    "encoded_input = tokenizer.encode_plus(\n",
    "    input_text,\n",
    "    add_special_tokens=True,\n",
    "    truncation=True,\n",
    "    padding='max_length',\n",
    "    max_length=128,\n",
    "    return_tensors='pt'\n",
    ")\n",
    "\n",
    "# Perform inference\n",
    "input_ids = encoded_input['input_ids']\n",
    "attention_mask = encoded_input['attention_mask']\n",
    "outputs = model(input_ids, attention_mask=attention_mask)\n",
    "\n",
    "# Get the predicted label\n",
    "predicted_label = outputs.logits.argmax().item()\n",
    "print('Predicted Label:', predicted_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
